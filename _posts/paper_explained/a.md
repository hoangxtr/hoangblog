# Feature-metric Loss for Self-supervised Learning of Depth and Egomotion

## Introduce
Depth estimation and Egomotion estimation from monocular camera là các task cơ bản và chủ đạo trong Computer vision. Chúng có thể được ứng dụng rộng rãi trong thực tế ảo, chuyển động robot, xe tự lái. Deeplearning nhìn chung có thể phân chúng thành 2 cách tiếp cận: Supervised và Self-supervised.  

Với supervised, Depth estimation data training sẽ là single ảnh và depth map tương ứng làm groundtruth. Depthmap có thể thu được bằng 2 cách: dùng sensor như LIDAR, cách này khá tốn chi phí và số lương thường không được nhiều, cách khác có thể thu được từ các simulation engines. Nhược điểm của dùng data từ sensor là không tổng quan, nghĩa là nó chỉ đúng với 1 tập data, 1 ngữ cảnh nào đó thôi, trong khi từ simulation engine thì thường không đúng thưc tế lắm. Với Egomotion Estimation (ước lượng chuyển động camera), data có thể tính toán được từ các phương pháp truyền thống với raw data lấy từ các sensor như [GPS](https://www.campbellsci.eu/gps#:~:text=GPS%20sensors%20are%20receivers%20with,%2C%20velocity%2C%20and%20timing%20information.) hoặc [IMU](https://en.wikipedia.org/wiki/Inertial_measurement_unit), Tuy nhiên cách này thường tốn kém chi phí và cũng không đảm bảo chính xác tuyệt đối.  

Với Self-supervised, 2 task kia được thống nhất thành 1 và được xử lí chung. Tuy nhiên, nó vẫn thua xa supervised khi được so sánh với cùng benchmark. Nguyên nhân là do việc sử dụng photometric loss vẫn chưa đủ tốt, nó vẫn chưa đủ để đảm bảo estimate depth and pose đúng (mặc đù loss nhỏ vẫn không chắc depth and pose như mong đợi), đặc biệt là ở các vùng low-texture (những vùng mà 2 frame không có sự khác biệt nhiều như bức tường, ngôi nhà, hàng rào bla bla. Vì những vùng vậy sẽ không tác động nhiều đến quá trình training). Mặc dù có thể dùng smoothloss để khắc phục phần nào photometric loss ở các vùng low-texture, nhưng vẫn không giúp được nhiều, ngoài ra có còn thể gây ra oversmooth tại các vùng boundaries.
Để giải quyết vấn đề của photometric loss, tác giả đã đề xuất feature-metric loss để tính toán dựa trên feature đã học được ứng với mỗi pixel thay vì như trên raw-pixel như photometric loss. Cụ thể hơn, để học được feature biểu diễn cho các pixel, tác giả sẽ đưa hình ảnh gốc qua 1 bộ auto-encoder để reconstruct lại nó. Để đảm bảo mạng auto-encoder hoạt động như mong muốn, tác giả sử dụng kèm 2 loss là discriminative loss and convergent loss. Discriminative loss sẽ đảm bảo không bị mất cạnh (vì nó khuyến khích mạng học các difference của 2 first-order của 2 ảnh). Trong khi convergent loss sẽ đảm bảo  *wide convergence basin* bằng cách penelize feature gradients’ variances across pixels.  

Nhìn chung paper sử dụng 3 sub-network: DepthNet, PoseNet và 1 net mới là FeatureNet để dùng riêng cho feature-metric loss.